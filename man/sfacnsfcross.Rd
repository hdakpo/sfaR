% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sfacnsfcross.R
\name{sfacnsfcross}
\alias{sfacnsfcross}
\alias{bread.sfacnsfcross}
\alias{estfun.sfacnsfcross}
\alias{print.sfacnsfcross}
\title{Contaminated noise stochastic frontier estimation using cross-sectional data}
\usage{
sfacnsfcross(
  formula,
  muhet,
  uhet,
  vhet,
  thet,
  logDepVar = TRUE,
  data,
  subset,
  weights,
  wscale = TRUE,
  S = 1L,
  udist = "hnormal",
  sigmauType = "common",
  linkF = "logit",
  start = NULL,
  randStart = FALSE,
  whichStart = 2L,
  initAlg = "nm",
  initIter = 100,
  method = "bfgs",
  hessianType = 1,
  simType = "halton",
  Nsim = 300,
  prime = 2L,
  burn = 10,
  antithetics = FALSE,
  seed = 12345,
  itermax = 2000L,
  printInfo = FALSE,
  tol = 1e-12,
  gradtol = 1e-06,
  stepmax = 0.1,
  qac = "marquardt"
)

\method{print}{sfacnsfcross}(x, ...)

bread.sfacnsfcross(x, ...)

estfun.sfacnsfcross(x, ...)
}
\arguments{
\item{formula}{A symbolic description of the model to be estimated based on
the generic function \code{formula} (see section \sQuote{Details}).}

\item{muhet}{A one-part formula to consider heterogeneity in the mean of the
pre-truncated distribution (see section \sQuote{Details}).}

\item{uhet}{A one-part formula to consider heteroscedasticity in the
one-sided error variance (see section \sQuote{Details}).}

\item{vhet}{A one-part formula to consider heteroscedasticity in the
two-sided error variance (see section \sQuote{Details}).}

\item{thet}{A one-part formula to account for inefficient and efficient
groups of observations (two classes).}

\item{logDepVar}{Logical. Informs whether the dependent variable is logged
(\code{TRUE}) or not (\code{FALSE}). Default = \code{TRUE}.}

\item{data}{The data frame containing the data.}

\item{subset}{An optional vector specifying a subset of observations to be
used in the optimization process.}

\item{weights}{An optional vector of weights to be used for weighted
log-likelihood. Should be \code{NULL} or numeric vector with positive values.
When \code{NULL}, a numeric vector of 1 is used.}

\item{wscale}{Logical. When \code{weights} is not \code{NULL}, a scaling
transformation is used such that the the \code{weights} sum to the sample
size. Default \code{TRUE}. When \code{FALSE} no scaling is used.}

\item{S}{If \code{S = 1} (default), a production (profit) frontier is
estimated: \eqn{\epsilon_i = v_i-u_i}. If \code{S = -1}, a cost frontier is
estimated: \eqn{\epsilon_i = v_i+u_i}.}

\item{udist}{Character string. Default = \code{'hnormal'}. Distribution
specification for the one-sided error term. 10 different distributions are
available: \itemize{ \item \code{'hnormal'}, for the half normal
distribution (Aigner \emph{et al.} 1977, Meeusen and Vandenbroeck 1977)
\item \code{'exponential'}, for the exponential distribution \item
\code{'tnormal'} for the truncated normal distribution (Stevenson 1980)
\item \code{'rayleigh'}, for the Rayleigh distribution (Hajargasht 2015)
\item \code{'uniform'}, for the uniform distribution (Li 1996, Nguyen 2010)
\item \code{'gamma'}, for the Gamma distribution (Greene 2003) \item
\code{'lognormal'}, for the log normal distribution (Migon and Medici 2001,
Wang and Ye 2020) \item \code{'weibull'}, for the Weibull distribution
(Tsionas 2007) \item \code{'genexponential'}, for the generalized
exponential distribution (Papadopoulos 2020) \item \code{'tslaplace'}, for
the truncated skewed Laplace distribution (Wang 2012). }}

\item{sigmauType}{Character string. Default = \code{'common'}. Nature of the
two-sided error term between classes. Two possibilities: \code{'common'} for
common error term \eqn{v} and \code{'different'} for different error term
\eqn{v}.}

\item{linkF}{Character string. Link function used for class membership
probability. Four possibilities are available: \code{logit} (default),
\code{probit}, \code{cauchit}, \code{cloglog}.}

\item{start}{Numeric vector. Optional starting values for the maximum
likelihood (ML) estimation.}

\item{randStart}{Logical. Define if random starting values should be used for
M(S)L estimation. New starting values are obtained as old ones + draws from
normal distribution with std. deviation of 0.01. \code{'seed'} is not
considered here, then each run will provide different starting values
(unless a seed is set by the user before the run).}

\item{whichStart}{Integer. If \code{'whichStart = 1'}, the starting values
are obtained from the method of moments. When \code{'whichStart = 2'}
(Default), the model is initialized by solving the homoscedastic pooled
cross section SFA model. \code{'whichStart = 1'} can be fast especially in
the case of maximum simulated likelihood.}

\item{initAlg}{Character string specifying the algorithm used for
initialization and obtain the starting values (when \code{'whichStart = 2'}).
Only \pkg{maxLik} package algorithms are available:
\itemize{ \item \code{'bfgs'}, for Broyden-Fletcher-Goldfarb-Shanno
(see \code{\link[maxLik:maxBFGS]{maxBFGS}})
\item \code{'bhhh'}, for Berndt-Hall-Hall-Hausman
(see \code{\link[maxLik:maxBHHH]{maxBHHH}})
\item \code{'nr'}, for Newton-Raphson (see
\code{\link[maxLik:maxNR]{maxNR}}) \item \code{'nm'}, for Nelder-Mead -
Default - (see \code{\link[maxLik:maxNM]{maxNM}}) \item \code{'cg'}, for
Conjugate Gradient (see \code{\link[maxLik:maxCG]{maxCG}})
\item \code{'sann'}, for Simulated Annealing (see
\code{\link[maxLik:maxSANN]{maxSANN}})
}}

\item{initIter}{Maximum number of iterations for initialization algorithm.
Default \code{100}.}

\item{method}{Optimization algorithm used for the estimation.  Default =
\code{'bfgs'}. 11 algorithms are available: \itemize{ \item \code{'bfgs'},
for Broyden-Fletcher-Goldfarb-Shanno (see
\code{\link[maxLik:maxBFGS]{maxBFGS}}) \item \code{'bhhh'}, for
Berndt-Hall-Hall-Hausman (see \code{\link[maxLik:maxBHHH]{maxBHHH}}) \item
\code{'nr'}, for Newton-Raphson (see \code{\link[maxLik:maxNR]{maxNR}})
\item \code{'nm'}, for Nelder-Mead (see \code{\link[maxLik:maxNM]{maxNM}})
\item \code{'cg'}, for Conjugate Gradient (see
\code{\link[maxLik:maxCG]{maxCG}}) \item \code{'sann'}, for Simulated
Annealing (see \code{\link[maxLik:maxSANN]{maxSANN}}) \item \code{'ucminf'},
for a quasi-Newton type optimization with BFGS updating of the inverse
Hessian and soft line search with a trust region type monitoring of
the input to the line search algorithm (see
\code{\link[ucminf:ucminf]{ucminf}})
\item \code{'mla'}, for general-purpose optimization based on
Marquardt-Levenberg algorithm (see \code{\link[marqLevAlg:mla]{mla}})
\item \code{'sr1'}, for Symmetric Rank 1 (see
\code{\link[trustOptim:trust.optim]{trust.optim}}) \item \code{'sparse'},
for trust regions and sparse Hessian (see
\code{\link[trustOptim:trust.optim]{trust.optim}}) \item
\code{'nlminb'}, for optimization using PORT routines (see
\code{\link[stats:nlminb]{nlminb}})}}

\item{hessianType}{Integer. If \code{1} (Default), analytic/numeric Hessian
is returned for all the distributions. If \code{2},  bhhh Hessian is
estimated (\eqn{g'g}).}

\item{simType}{Character string. If \code{simType = 'halton'} (Default),
Halton draws are used for maximum simulated likelihood (MSL). If
\code{simType = 'ghalton'}, or \code{simType = 'rhalton'}, Generalized-Halton
draws or randomized Halton draws are used for MSL. If
\code{simType = 'sobol'} or \code{simType = 'rsobol'}, Sobol draws or
randomized Sobol draws are used for MSL, respectively. If
\code{simType = 'richtmyer'}, or \code{simType = 'rrichtmyer'},
Richtmyer sequence or randomized Richtmyer sequence is used for MSL
estimation, respectively. If \code{simType = 'uniform'}, uniform draws are
used for MSL. (see section \sQuote{Details}).}

\item{Nsim}{Number of draws for MSL.}

\item{prime}{Prime number considered for Halton and Generalized-Halton
draws. Default = \code{2}.}

\item{burn}{Number of the first observations discarded in the case of Halton
draws. Default = \code{10}.}

\item{antithetics}{Logical. Default = \code{FALSE}. If \code{TRUE},
antithetics counterpart of the uniform draws is computed. (see section
\sQuote{Details}).}

\item{seed}{Numeric. Seed for the random draws.}

\item{itermax}{Maximum number of iterations allowed for optimization.
Default = \code{2000}.}

\item{printInfo}{Logical. Print information during optimization. Default =
\code{FALSE}.}

\item{tol}{Numeric. Convergence tolerance. Default = \code{1e-12}.}

\item{gradtol}{Numeric. Convergence tolerance for gradient. Default =
\code{1e-06}.}

\item{stepmax}{Numeric. Step max for \code{ucminf} algorithm. Default =
\code{0.1}.}

\item{qac}{Character. Quadratic Approximation Correction for \code{'bhhh'}
and \code{'nr'} algorithms. If \code{'stephalving'}, the step length is
decreased but the direction is kept. If \code{'marquardt'} (default), the
step length is decreased while also moving closer to the pure gradient
direction. See \code{\link[maxLik:maxBHHH]{maxBHHH}} and
\code{\link[maxLik:maxNR]{maxNR}}.}

\item{x}{an object of class sfacnsfcross (returned by the function
\code{\link{sfacnsfcross}}).}

\item{...}{additional arguments of frontier are passed to sfacnsfcross;
additional arguments of the print, bread, estfun, nobs methods are currently
ignored.}
}
\value{
\code{\link{sfacnsfcross}} returns a list of class
\code{'sfacnsfcross'} containing the following elements:

\item{call}{The matched call.}

\item{formula}{The estimated model.}

\item{S}{The argument \code{'S'}. See the section \sQuote{Arguments}.}

\item{typeSfa}{Character string. 'Contaminated Noise Production/Profit
Frontier, e = v - u' when \code{S = 1} and 'Contaminated Noise Cost Frontier,
e = v + u' when \code{S = -1}.}

\item{Nobs}{Number of observations used for optimization.}

\item{nXvar}{Number of explanatory variables in the production or cost
frontier.}

\item{nmuZUvar}{Number of variables explaining heterogeneity in the
truncated mean, only if \code{udist = 'tnormal'} or \code{'lognormal'}.}

\item{logDepVar}{The argument \code{'logDepVar'}. See the section
\sQuote{Arguments}.}

\item{nuZUvar}{Number of variables explaining heteroscedasticity in the
one-sided error term.}

\item{nvZVvar}{Number of variables explaining heteroscedasticity in the
two-sided error term.}

\item{nParm}{Total number of parameters estimated.}

\item{udist}{The argument \code{'udist'}. See the section
\sQuote{Arguments}.}

\item{startVal}{Numeric vector. Starting value for M(S)L estimation.}

\item{dataTable}{A data frame (tibble format) containing information on data
used for optimization along with residuals and fitted values of the OLS and
M(S)L estimations, and the individual observation log-likelihood. When
\code{weights} is specified an additional variable is also provided in
\code{dataTable}.}

\item{sigmauType}{Character string. Nature of the
two-sided error term between classes. See \strong{Arguments} section.}

\item{linkF}{Character string. Link function retained to define
class membership probability. See \strong{Arguments} section.}

\item{initHalf}{When \code{start = NULL}, \code{whichStart == 2L}, and
\code{udist = 'hnormal'}. Initial ML estimation with half normal distribution
for the one-sided error term. Model to construct the starting values for
the cnsf estimation.
Object of class \code{'maxLik'} and \code{'maxim'} returned.
For \code{udist = 'exponential'} it returns \code{initExpo},
for \code{udist = 'tnormal'} it returns \code{initTrunc},
for \code{udist = 'rayleigh'} it returns \code{initRay},
for \code{udist = 'uniform'} it returns \code{initUni},
for \code{udist = 'gamma'} it returns \code{initGamma},
for \code{udist = 'lognormal'} it returns \code{initLog},
for \code{udist = 'genexponential'} it returns \code{initGenexpo},
for \code{udist = 'tslaplace'} it returns \code{initTSL}.}

\item{isWeights}{Logical. If \code{TRUE} weighted log-likelihood is
maximized.}

\item{optType}{Optimization algorithm used.}

\item{nIter}{Number of iterations of the ML estimation.}

\item{optStatus}{Optimization algorithm termination message.}

\item{startLoglik}{Log-likelihood at the starting values.}

\item{mlLoglik}{Log-likelihood value of the M(S)L estimation.}

\item{mlParam}{Parameters obtained from M(S)L estimation.}

\item{gradient}{Each variable gradient of the M(S)L estimation.}

\item{gradL_OBS}{Matrix. Each variable individual observation gradient of
the M(S)L estimation.}

\item{gradientNorm}{Gradient norm of the M(S)L estimation.}

\item{invHessian}{Covariance matrix of the parameters obtained from the
M(S)L estimation.}

\item{conditionNums}{Matrix. Condition number adding columns one by one.}

\item{hessianType}{The argument \code{'hessianType'}. See the section
\sQuote{Arguments}.}

\item{mlDate}{Date and time of the estimated model.}

\item{simDist}{The argument \code{'simDist'}, only if \code{udist =
'gamma'}, \code{'lognormal'} or , \code{'weibull'}. See the section
\sQuote{Arguments}.}

\item{Nsim}{The argument \code{'Nsim'}, only if \code{udist = 'gamma'},
\code{'lognormal'} or , \code{'weibull'}. See the section
\sQuote{Arguments}.}

\item{FiMat}{Matrix of random draws used for MSL, only if \code{udist =
'gamma'}, \code{'lognormal'} or , \code{'weibull'}.}
}
\description{
\code{\link{sfacnsfcross}} is a symbolic formula-based function for the
estimation of stochastic frontier models in the presence of efficient and
inefficient groups of observation, in the case of cross-sectional or
pooled cross-sectional data, using maximum (simulated) likelihood - M(S)L.

The function accounts for heteroscedasticity in both one-sided and two-sided
error terms as in Reifschneider and Stevenson (1991), Caudill and Ford
(1993), Caudill \emph{et al.} (1995) and Hadri (1999), but also
heterogeneity in the mean of the pre-truncated distribution as in Kumbhakar
\emph{et al.} (1991), Huang and Liu (1994) and Battese and Coelli (1995).

Ten distributions are possible for the one-sided error term and nine
optimization algorithms are available.
}
\details{
Following Wheat \emph{et al.} (2019) the model can be defined as:

\deqn{y_i = \alpha + \mathbf{x_i^{\prime}}\bm{\beta} + 
v_{1i} - Su_i \quad \hbox{with probability} \quad p}

\deqn{y_i = \alpha + \mathbf{x_i^{\prime}}\bm{\beta} + 
v_{2i} - Su_i \quad \hbox{with probability} \quad 1-p}

where \eqn{i} is the observation, \eqn{y} is the
output (cost, revenue, profit), \eqn{x} is the vector of main explanatory
variables (inputs and other control variables), \eqn{u} is the one-sided
error term with variance \eqn{\sigma_{u}^2}, and \eqn{v_1} and \eqn{v_2} are
the two-sided error terms with variance \eqn{\sigma_{v_1}^2} and
\eqn{\sigma_{v_2}^2} respectively.

\code{S = 1} in the case of production (profit) frontier function and
\code{S = -1} in the case of cost frontier function.

The prior probability of belonging to class \eqn{1} can depend on some
covariates using a logit specification:

\deqn{p_i = \frac{\exp{(\mathbf{Z}_{hi}^{\prime}\bm{\theta})}}{1-
\exp{(\mathbf{Z}_{hi}^{\prime}\bm{\theta})}}}

with \eqn{\mathbf{Z}_h} the covariates, \eqn{\bm{\theta}} the vector of
coefficients estimated for the covariates.

Other link functions are also available. In the case of the probit we have:

\deqn{p_i = \Phi\left(\mathbf{Z}_{hi}^{\prime}\bm{\theta}\right)}

when the cauchit link is retained we have:

\deqn{p_i = 1/\pi\arctan(\mathbf{Z}_{hi}^{\prime}\bm{\theta})+1/2}

and finally in the case of the cloglog link we have:

\deqn{p_i = 1-\exp\left(-\exp(\mathbf{Z}_{hi}^{\prime}
\bm{\theta})\right)}

Let

\deqn{\epsilon_i = v_i -Su_i}

In the case of the truncated normal distribution, the convolution of
\eqn{u_i} and \eqn{v_i} is:

\deqn{f(\epsilon_i)=\frac{p_i}{\sqrt{\sigma_u^2 + 
\sigma_{v_1^2}}}\phi\left(\frac{S\epsilon_i + \mu}{\sqrt{
\sigma_u^2 + \sigma_{v_1^2}}}\right)\Phi\left(\frac{
\mu_{1i\ast}}{\sigma_{1\ast}}\right) \Big/\Phi\left(
\frac{\mu}{\sigma_u}\right) + \frac{1-p_i}{\sqrt{\sigma_u^2 + 
\sigma_{v_2^2}}}\phi\left(\frac{S\epsilon_i + \mu}{\sqrt{
\sigma_u^2 + \sigma_{v_2^2}}}\right)\Phi\left(\frac{
\mu_{2i\ast}}{\sigma_{2\ast}}\right) \Big/\Phi\left(
\frac{\mu}{\sigma_u}\right)}

where

\deqn{\mu_{1i*}=\frac{\mu\sigma_{v_1^2} - 
S\epsilon_i\sigma_u^2}{\sigma_u^2 + \sigma_{v_1^2}}}

and

\deqn{\sigma_{1*}^2 = \frac{\sigma_u^2 
\sigma_{v_1^2}}{\sigma_u^2 + \sigma_{v_1^2}}}

\eqn{\mu_{2i*}} and \eqn{\sigma_{2*}^2} are similarly obtained.

In the case of the half normal distribution the convolution is obtained by
setting \eqn{\mu=0}.

Class assignment is based on the largest posterior probability. This
probability is obtained using Bayes' rule, as follows for the inefficient
class

\deqn{p_i^\ast = \frac{p_i \times 
\pi(i, 1)}{f(\epsilon_i)}}

where

\deqn{\pi(i, 1)=\frac{1}{\sqrt{\sigma_u^2 + 
\sigma_v^2}}\phi\left(\frac{S\epsilon_i + \mu}{\sqrt{
\sigma_u^2 + \sigma_v^2}}\right)\Phi\left(\frac{
\mu_{i\ast}}{\sigma_\ast}\right)\Big/\Phi\left(
\frac{\mu}{\sigma_u}\right)}

The model presented so far assumed common inefficiency term \eqn{u} for the
two classes of observations. By setting argument
\code{sigmauType = 'different'}, different inefficiency terms are imposed for
the two groups. The new estimated model is presented as follows:

\deqn{y_i = \alpha + \mathbf{x_i^{\prime}}\bm{\beta} + 
v_{1i} - Su_{1i} \quad \hbox{with probability} \quad p}

\deqn{y_i = \alpha + \mathbf{x_i^{\prime}}\bm{\beta} + 
v_{2i} - Su_{2i} \quad \hbox{with probability} \quad 1-p}

\code{sfacnsfcross} allows for the maximization of weighted log-likelihood.
When option \code{weights} is specified and \code{wscale = TRUE}, the weights
are scaled as

\deqn{new_{weights} = sample_{size} \times 
\frac{old_{weights}}{\sum(old_{weights})}}

For complex problems, non-gradient methods (e.g. \code{nm} or \code{sann})
can be used to warm start the optimization and zoom in the neighborhood of
the solution. Then a gradient-based methods is recommended in the second
step. In the case of \code{sann}, we recommend to significantly increase the
iteration limit (e.g. \code{itermax = 20000}). The Conjugate Gradient
(\code{cg}) can also be used in the first stage.

A set of extractor functions for fitted model objects is available for
objects of class \code{'sfacnsfcross'} including methods to the generic
functions \code{\link[=print.sfacnsfcross]{print}},
\code{\link[=summary.sfacnsfcross]{summary}},
\code{\link[=coef.sfacnsfcross]{coef}},
\code{\link[=fitted.sfacnsfcross]{fitted}},
\code{\link[=logLik.sfacnsfcross]{logLik}},
\code{\link[=residuals.sfacnsfcross]{residuals}},
\code{\link[=vcov.sfacnsfcross]{vcov}},
\code{\link[=efficiencies.sfacnsfcross]{efficiencies}},
\code{\link[=ic.sfacnsfcross]{ic}},
\code{\link[=marginal.sfacnsfcross]{marginal}},
\code{\link[=estfun.sfacnsfcross]{estfun}} and
\code{\link[=bread.sfacnsfcross]{bread}} (from the \CRANpkg{sandwich}
package), \code{\link[lmtest:coeftest]{lmtest::coeftest()}} (from the \CRANpkg{lmtest} package).
}
\note{
For the Halton draws, the code is adapted from the \pkg{mlogit}
package.
}
\examples{

# Using data on Spanish dairy farms
## Cobb Douglas (production function) half normal distribution

cb_cnsf_h <- sfacnsfcross(formula = YIT ~ X1 + X2 + X3 + X4, 
udist = 'hnormal', data = dairyspain, S = 1, method = 'bfgs')

summary(cb_cnsf_h)

}
\references{
Aigner, D., Lovell, C. A. K., and Schmidt, P. 1977. Formulation
and estimation of stochastic frontier production function models.
\emph{Journal of Econometrics}, \bold{6}(1), 21--37.

Battese, G. E., and Coelli, T. J. 1995. A model for technical inefficiency
effects in a stochastic frontier production function for panel data.
\emph{Empirical Economics}, \bold{20}(2), 325--332.

Caudill, S. B., and Ford, J. M. 1993. Biases in frontier estimation due to
heteroscedasticity. \emph{Economics Letters}, \bold{41}(1), 17--20.

Caudill, S. B., Ford, J. M., and Gropper, D. M. 1995. Frontier estimation
and firm-specific inefficiency measures in the presence of
heteroscedasticity. \emph{Journal of Business & Economic Statistics},
\bold{13}(1), 105--111.

Greene, W. H. 2003. Simulated likelihood estimation of the normal-Gamma
stochastic frontier function. \emph{Journal of Productivity Analysis},
\bold{19}(2-3), 179--190.

Hadri, K. 1999. Estimation of a doubly heteroscedastic stochastic frontier
cost function. \emph{Journal of Business & Economic Statistics},
\bold{17}(3), 359--363.

Hajargasht, G. 2015. Stochastic frontiers with a Rayleigh distribution.
\emph{Journal of Productivity Analysis}, \bold{44}(2), 199--208.

Huang, C. J., and Liu, J.-T. 1994. Estimation of a non-neutral stochastic
frontier production function. \emph{Journal of Productivity Analysis},
\bold{5}(2), 171--180.

Kumbhakar, S. C., Parmeter, C. F., and Tsionas, E. G. 2013)
A zero inefficiency stochastic frontier model.
\emph{Journal of Econometrics}, \bold{172}(1), 66--76.

Li, Q. 1996. Estimating a stochastic production frontier when the adjusted
error is symmetric. \emph{Economics Letters}, \bold{52}(3), 221--228.

Meeusen, W., and Vandenbroeck, J. 1977. Efficiency estimation from
Cobb-Douglas production functions with composed error. \emph{International
Economic Review}, \bold{18}(2), 435--445.

Migon, H. S., and Medici, E. V. 2001. Bayesian hierarchical models for
stochastic production frontier. Lacea, Montevideo, Uruguay.

Nguyen, N. B. 2010. Estimation of technical efficiency in stochastic
frontier analysis. PhD dissertation, Bowling Green State University, August.

Papadopoulos, A. 2021. Stochastic frontier models using the generalized
exponential distribution. \emph{Journal of Productivity Analysis},
\bold{55}:15--29.

Reifschneider, D., and Stevenson, R. 1991. Systematic departures from the
frontier: A framework for the analysis of firm inefficiency.
\emph{International Economic Review}, \bold{32}(3), 715--723.

Stevenson, R. E. 1980. Likelihood Functions for Generalized Stochastic
Frontier Estimation. \emph{Journal of Econometrics}, \bold{13}(1), 57--66.

Tsionas, E. G. 2007. Efficiency measurement with the Weibull stochastic
frontier. \emph{Oxford Bulletin of Economics and Statistics}, \bold{69}(5),
693--706.

Wang, K., and Ye, X. 2020. Development of alternative stochastic frontier
models for estimating time-space prism vertices. \emph{Transportation}.

Wang, J. 2012. A normal truncated skewed-Laplace model in stochastic
frontier analysis. Master thesis, Western Kentucky University, May.

Wheat, P., Stead, A. D., & Greene, W. H. 2019. Controlling for Outliers in
Efficiency Analysis: A Contaminated Normal-Half Normal Stochastic Frontier
Model. Working Paper.
}
\seealso{
\code{\link[=print.sfacnsfcross]{print}} for printing
\code{sfacnsfcross} object.

\code{\link[=summary.sfacnsfcross]{summary}} for creating and printing
summary results.

\code{\link[=coef.sfacnsfcross]{coef}} for extracting coefficients of the
estimation.

\code{\link[=efficiencies.sfacnsfcross]{efficiencies}} for computing
(in-)efficiency estimates.

\code{\link[=fitted.sfacnsfcross]{fitted}} for extracting the fitted frontier
values.

\code{\link[=ic.sfacnsfcross]{ic}} for extracting information criteria.

\code{\link[=logLik.sfacnsfcross]{logLik}} for extracting log-likelihood
value(s) of the estimation.

\code{\link[=marginal.sfacnsfcross]{marginal}} for computing marginal effects
of inefficiency drivers.

\code{\link[=residuals.sfacnsfcross]{residuals}} for extracting residuals of
the estimation.

\code{\link[=vcov.sfacnsfcross]{vcov}} for computing the variance-covariance
matrix of the coefficients.

\code{\link[=bread.sfacnsfcross]{bread}} for bread for sandwich estimator.

\code{\link[=estfun.sfacnsfcross]{estfun}} for gradient extraction for each
observation.
}
\keyword{cross-section}
\keyword{likelihood}
\keyword{models}
\keyword{optimize}
