% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sfapanel1.R
\name{sfapanel1}
\alias{sfapanel1}
\alias{bread.sfapanel1}
\alias{estfun.sfapanel1}
\alias{print.sfapanel1}
\title{Stochastic frontier estimation using panel data}
\usage{
sfapanel1(
  formula,
  muhet,
  uhet,
  vhet,
  logDepVar = TRUE,
  data,
  idVar = NULL,
  timeVar = NULL,
  subset,
  weights,
  wscale = TRUE,
  S = 1L,
  modelType = "bc92a",
  udist = "hnormal",
  start = NULL,
  randStart = FALSE,
  whichStart = 2L,
  initAlg = "nm",
  initIter = 100,
  invariance = 2L,
  method = "bfgs",
  hessianType = 1L,
  simType = "halton",
  Nsim = 100,
  prime = 2L,
  burn = 10,
  antithetics = FALSE,
  seed = 12345,
  itermax = 2000,
  printInfo = FALSE,
  tol = 1e-12,
  gradtol = 1e-06,
  stepmax = 0.1,
  qac = "marquardt"
)

\method{print}{sfapanel1}(x, ...)

bread.sfapanel1(x, ...)

estfun.sfapanel1(x, ...)
}
\arguments{
\item{formula}{A symbolic description of the model to be estimated based on
the generic function \code{formula} (see section \sQuote{Details}).}

\item{muhet}{A one-part formula to consider heterogeneity in the mean of the
pre-truncated distribution (see section \sQuote{Details}).}

\item{uhet}{A one-part formula to consider heteroscedasticity in the
one-sided error variance (see section \sQuote{Details}).}

\item{vhet}{A one-part formula to consider heteroscedasticity in the
two-sided error variance (see section \sQuote{Details}).}

\item{logDepVar}{Logical. Informs whether the dependent variable is logged
(\code{TRUE}) or not (\code{FALSE}). Default = \code{TRUE}.}

\item{data}{The data frame containing the data. The data can be an object of
class \code{'pdata.frame'}.}

\item{idVar}{Character. When \code{'data'} is not an object of class
\code{'pdata.frame'}, \code{'idVar'} defines the cross sections
identification. Default = \code{NULL}.}

\item{timeVar}{Character. When \code{'data'} is not an object of class
\code{'pdata.frame'}, \code{'timeVar'} defines the periods identification.
Default = \code{NULL}.}

\item{subset}{An optional vector specifying a subset of observations to be
used in the optimization process.}

\item{weights}{An optional vector of weights to be used for weighted
log-likelihood. Should be \code{NULL} or numeric vector with positive values.
When \code{NULL}, a numeric vector of 1 is used.}

\item{wscale}{Logical. When \code{weights} is not \code{NULL}, a scaling
transformation is used such that the the \code{weights} sum to the sample
size. Default = \code{TRUE}. When \code{FALSE} no scaling is used.}

\item{S}{If \code{S = 1} (default), a production (profit) frontier is
estimated. If \code{S = -1}, a cost frontier is estimated.}

\item{modelType}{Character string specifying the panel model to be estimated.
See \sQuote{Details} section.
\itemize{
\item Default = \code{'bc92a'} for the model discussed in Battese and Coelli
(1992).
\item A modified version (\code{'mbc92'}) of the previous model was also
presented in Battese and Coelli (1992).
\item \code{'bc92b'} implements the model presented in Cuesta and Orea (2002),
and Feng and Serletis (2009).
\item \code{'bc92c'} for the model in Alvarez \emph{et al.} (2006).
\item \code{'c00'} for the model in Cuesta (2000).
\item \code{'kw05'} for the model in Kumbhakar and Wang (2005).
\item \code{'mols93'} for the modified version of the model in
Lee and Schmidt (1993).
\item \code{'pl81'} for the time invariant inefficiency in
Pitt and Lee (1981).
}}

\item{udist}{Character string. Default = \code{'hnormal'}. Distribution
specification for the one-sided error term. 10 different distributions are
available: \itemize{ \item \code{'hnormal'}, for the half normal
distribution (Aigner \emph{et al.} 1977, Meeusen and Vandenbroeck 1977)
\item \code{'exponential'}, for the exponential distribution \item
\code{'tnormal'} for the truncated normal distribution (Stevenson 1980)
\item \code{'rayleigh'}, for the Rayleigh distribution (Hajargasht 2015)
\item \code{'uniform'}, for the uniform distribution (Li 1996, Nguyen 2010)
\item \code{'gamma'}, for the Gamma distribution (Greene 2003) \item
\code{'lognormal'}, for the log normal distribution (Migon and Medici 2001,
Wang and Ye 2020) \item \code{'weibull'}, for the Weibull distribution
(Tsionas 2007) \item \code{'genexponential'}, for the generalized
exponential distribution (Papadopoulos 2020) \item \code{'tslaplace'}, for
the truncated skewed Laplace distribution (Wang 2012). }}

\item{start}{Numeric vector. Optional starting values for the maximum
likelihood (ML) estimation.}

\item{randStart}{Logical. Define if random starting values should be used for
M(S)L estimation. New starting values are obtained as old ones + draws from
normal distribution with std. deviation of 0.01. \code{'seed'} is not
considered here, then each run will provide different starting values
(unless a seed is set by the user before the run).}

\item{whichStart}{Integer. If \code{'whichStart = 1'}, the starting values
are obtained from the method of moments applied to the pooled data.
When \code{'whichStart = 2'} (Default), the model is initialized by solving
the classic SFA model for the homoscedastic pooled cross section data.
\code{'whichStart = 1'} can be fast especially in the case of maximum
simulated likelihood.}

\item{initAlg}{Character string specifying the algorithm used for
initializing and obtaining the starting values (when \code{'whichStart = 2'}).
Only \pkg{maxLik} package algorithms are available:
\itemize{ \item \code{'bfgs'}, for Broyden-Fletcher-Goldfarb-Shanno
(see \code{\link[maxLik:maxBFGS]{maxBFGS}})
\item \code{'bhhh'}, for Berndt-Hall-Hall-Hausman
(see \code{\link[maxLik:maxBHHH]{maxBHHH}})
\item \code{'nr'}, for Newton-Raphson (see \code{\link[maxLik:maxNR]{maxNR}})
\item \code{'nm'}, for Nelder-Mead - Default -
(see \code{\link[maxLik:maxNM]{maxNM}})
\item \code{'cg'}, for Conjugate Gradient
(see \code{\link[maxLik:maxCG]{maxCG}}) \item \code{'sann'}, for Simulated
Annealing (see \code{\link[maxLik:maxSANN]{maxSANN}})
}}

\item{initIter}{Maximum number of iterations for initializing algorithm.
Default \code{100}.}

\item{invariance}{Integer. Except in the case of \code{'modelType = 'bc92c''},
in the presence of heteroscedasticity, variables are supposed to be constant
over time for each cross section. When this is not the case, three options
are suggested to create constant variables: when \code{'invariance = 1L'},
the first period observation is used; when \code{'invariance = 2L'}, the last
period observation is used; and when \code{'invariance = 3L'}, the period
mean is used. The \code{'invariance'} option also applies to the
\code{'weights'} variable.}

\item{method}{Optimization algorithm used for the estimation. Default =
\code{'bfgs'}. 11 algorithms are available: \itemize{ \item \code{'bfgs'},
for Broyden-Fletcher-Goldfarb-Shanno (see
\code{\link[maxLik:maxBFGS]{maxBFGS}}) \item \code{'bhhh'}, for
Berndt-Hall-Hall-Hausman (see \code{\link[maxLik:maxBHHH]{maxBHHH}}) \item
\code{'nr'}, for Newton-Raphson (see \code{\link[maxLik:maxNR]{maxNR}})
\item \code{'nm'}, for Nelder-Mead (see \code{\link[maxLik:maxNM]{maxNM}})
\item \code{'cg'}, for Conjugate Gradient
(see \code{\link[maxLik:maxCG]{maxCG}}) \item \code{'sann'}, for Simulated
Annealing (see \code{\link[maxLik:maxSANN]{maxSANN}}) \item \code{'ucminf'},
for a quasi-Newton type optimization with BFGS updating of the inverse
Hessian and soft line search with a trust region type monitoring of the input
to the line search algorithm (see \code{\link[ucminf:ucminf]{ucminf}})
\item \code{'mla'}, for general-purpose optimization based on
Marquardt-Levenberg algorithm (see \code{\link[marqLevAlg:mla]{mla}})
\item \code{'sr1'}, for Symmetric Rank 1 (see
\code{\link[trustOptim:trust.optim]{trust.optim}}) \item \code{'sparse'},
for trust regions and sparse Hessian
(see \code{\link[trustOptim:trust.optim]{trust.optim}}) \item
\code{'nlminb'}, for optimization using PORT routines (see
\code{\link[stats:nlminb]{nlminb}})}}

\item{hessianType}{Integer. If \code{1} (Default), analytic/numeric Hessian
is returned for all the distributions. If \code{2}, bhhh Hessian is
estimated (\eqn{g'g}).}

\item{simType}{Character string. If \code{simType = 'halton'} (Default),
Halton draws are used for maximum simulated likelihood (MSL). If
\code{simType = 'ghalton'}, Generalized-Halton  draws are used for MSL. If
\code{simType = 'sobol'} or \code{simType = 'rsobol'}, Sobol draws or
randomized Sobol draws are used for MSL, respectively. If
\code{simType = 'richtmyer'}, or \code{simType = 'rrichtmyer'},
Richtmyer sequence or randomized Richtmyer sequence is used for MSL
estimation, respectively. If \code{simType = 'mlhs'}, modified latin
hypercube sampling is performed. If \code{simType = 'uniform'}, uniform draws
are used for MSL. (see section \sQuote{Details}).}

\item{Nsim}{Number of draws for MSL. Default 100.}

\item{prime}{Prime number considered for quasi-random sequences. The first
10,000 primes are available. Default = \code{2}.}

\item{burn}{Number of the first observations discarded in the case of Halton,
Richtmyer, and Sobol draws. For the randomized versions, the burn is not
considered. Default = \code{10}.}

\item{antithetics}{Logical. Default = \code{FALSE}. If \code{TRUE},
antithetics counterpart of the uniform draws is computed. (see section
\sQuote{Details}).}

\item{seed}{Numeric. Seed for the random draws.}

\item{itermax}{Maximum number of iterations allowed for optimization.
Default = \code{2000}.}

\item{printInfo}{Logical. Print information during optimization. Default =
\code{FALSE}.}

\item{tol}{Numeric. Convergence tolerance. Default = \code{1e-12}.}

\item{gradtol}{Numeric. Convergence tolerance for gradient. Default =
\code{1e-06}.}

\item{stepmax}{Numeric. Step max for \code{ucminf} algorithm. Default =
\code{0.1}.}

\item{qac}{Character. Quadratic Approximation Correction for \code{'bhhh'}
and \code{'nr'} algorithms. If \code{'stephalving'}, the step length is
decreased but the direction is kept. If \code{'marquardt'} (default), the
step length is decreased while also moving closer to the pure gradient
direction. See \code{\link[maxLik:maxBHHH]{maxBHHH}} and
\code{\link[maxLik:maxNR]{maxNR}}.}

\item{x}{an object of class sfapanel1 (returned by the function
\code{\link{sfapanel1}}).}

\item{...}{additional arguments of frontier are passed to sfapanel;
additional arguments of the print, bread, estfun, nobs methods are currently
ignored.}
}
\value{
\code{\link{sfapanel1}} returns a list of class \code{'sfapanel1'}
containing the following elements:

\item{call}{The matched call.}

\item{formula}{The estimated model.}

\item{S}{The argument \code{'S'}. See the section \sQuote{Arguments}.}

\item{typeSfa}{Character string. 'Stochastic Production/Profit Frontier, e =
v - u' when \code{S = 1} and 'Stochastic Cost Frontier, e = v + u' when
\code{S = -1}.}

\item{Nobs}{Total number of observations.}

\item{Nid}{Number of cross-sections in the panel data.}

\item{Vtime}{Vector of total number of periods of presence of each
cross-section.}

\item{Ntime}{Average periods of presence of each cross-section.}

\item{nXvar}{Number of explanatory variables in the production or cost
frontier.}

\item{nmuZUvar}{Number of variables explaining heterogeneity in the
truncated mean, only if \code{udist = 'tnormal'} or \code{'lognormal'}.}

\item{nuZUvar}{Number of variables explaining heteroscedasticity in the
one-sided error term.}

\item{nvZVvar}{Number of variables explaining heteroscedasticity in the
two-sided error term.}

\item{logDepVar}{The argument \code{'logDepVar'}. See the section
\sQuote{Arguments}.}

\item{nParm}{Total number of parameters estimated.}

\item{udist}{The argument \code{'udist'}. See the section
\sQuote{Arguments}.}

\item{startVal}{Numeric vector. Starting value for M(S)L estimation.}

\item{dataTable}{A data frame (tibble format) containing information on data
used for optimization along with residuals and fitted values of the OLS and
M(S)L estimations, and the individual observation log-likelihood. When
\code{weights} is specified an additional variable is also provided in
\code{dataTable}.}

\item{modelType}{Spefication used for the panel model.}

\item{gHvar}{Matrix of variables used in the case of time varying
inefficiency when \code{'modelType %in% c('bc92a', 'bc92b', 'bc92c', 'kw05',
'c00', 'mols93')'} (for internal use).}

\item{invariance}{Methodology used to obtain time invariant exogenous
variables (in the case of heteroscedasticity).}

\item{initHalf}{When \code{'whichStart = 2L'} and \code{'udist = 'hnormal''}.
Initial ML estimation with half normal distribution for the one-sided error
term. Model to construct the starting values. In the case of other
distributions, we have: \code{'initExpo'} for \code{'udist = 'exponential''};
\code{'initTrunc'} for \code{'udist = 'tnormal''};
\code{'initRay'} for \code{'udist = 'rayleigh''}
\code{'initUni'} for \code{'udist = 'uniform''}
\code{'initGamma'} for \code{'udist = 'gamma''}
\code{'initLog'} for \code{'udist = 'lognormal''}
\code{'initWeibull'} for \code{'udist = 'weibull''}
\code{'initGenExpo'} for \code{'udist = 'genexponential''}
\code{'initTSL'} for \code{'udist = 'tslaplace''}. Object of class 'maxLik'
and 'maxim' returned.}

\item{isWeights}{Logical. If \code{TRUE} weighted log-likelihood is
maximized.}

\item{optType}{Optimization algorithm used.}

\item{nIter}{Number of iterations of the ML estimation.}

\item{optStatus}{Optimization algorithm termination message.}

\item{startLoglik}{Log-likelihood at the starting values.}

\item{mlLoglik}{Log-likelihood value of the M(S)L estimation.}

\item{mlParam}{Parameters obtained from M(S)L estimation.}

\item{gradient}{Each variable gradient of the M(S)L estimation.}

\item{gradL_OBS}{Matrix. Each variable individual observation gradient of
the M(S)L estimation.}

\item{gradientNorm}{Gradient norm of the M(S)L estimation.}

\item{invHessian}{Covariance matrix of the parameters obtained from the
M(S)L estimation.}

\item{conditionNums}{Matrix. Condition number adding columns one by one.}

\item{hessianType}{The argument \code{'hessianType'}. See the section
\sQuote{Arguments}.}

\item{mlDate}{Date and time of the estimated model.}

\item{simDist}{The argument \code{'simDist'}, only if \code{udist =
'gamma'}, \code{'lognormal'} or , \code{'weibull'}. See the section
\sQuote{Arguments}.}

\item{Nsim}{The argument \code{'Nsim'}, only if \code{udist = 'gamma'},
\code{'lognormal'} or , \code{'weibull'}. See the section
\sQuote{Arguments}.}

\item{FiMat}{Matrix of random draws used for MSL, only if \code{udist =
'gamma'}, \code{'lognormal'} or , \code{'weibull'}.}
}
\description{
\code{\link{sfapanel1}} is a symbolic formula-based function for the
estimation of stochastic frontier models in the case of panel data, using
maximum (simulated) likelihood - M(S)L. Several panel models are implemented:
the time invariant inefficiency model discussed in Pitt and Lee (1981), the
time varying efficiency models suggested in Kumbhakar (1990),
Battese and Coelli (1992), Cuesta (2000), Cuesta and Orea (2002),
Kumbhakar and Wang (2005), Alvarez \emph{et al.} (2006),
Feng and Serletis (2009). We also suggested a modified version of the model
by Lee and Schmidt (1993) - see \sQuote{Details} section.

Depending on the specification, the function accounts for heteroscedasticity
in both one-sided and two-sided error terms as in
Reifschneider and Stevenson (1991), Caudill and Ford (1993),
Caudill \emph{et al.} (1995) and Hadri (1999), but also
heterogeneity in the mean of the pre-truncated distribution as in Kumbhakar
\emph{et al.} (1991), Huang and Liu (1994) and Battese and Coelli (1995).
Alvarez \emph{et al.} (2006) implements a version of the time varying
inefficiency using the scaling property as in Wang and Schmidt (2002).

Ten distributions are possible for the one-sided error term and eleven
optimization algorithms are available.
}
\details{
The stochastic frontier model for the panel data in the case of the time
invariant inefficiency model (Pitt and Lee (1981): \code{'pl81'}) is defined
as:

\deqn{y_{it} = \alpha + \mathbf{x_{it}^{\prime}}\bm{\beta} +  v_{it} - Su_i}

with

\deqn{\epsilon_{it} = v_{it} -Su_i}

where \eqn{i} is the cross section and \eqn{t} the period the cross section
is observed, \eqn{y} is the output (cost, revenue, profit), \eqn{\mathbf{x}}
is the vector of main explanatory variables (inputs and other control
variables), \eqn{u} is the one-sided error term with variance
\eqn{\sigma_{u}^2}, and \eqn{v} is the two-sided error term with variance
\eqn{\sigma_{v}^2}.

\code{S = 1} in the case of production (profit) frontier function and
\code{S = -1} in the case of cost frontier function.

All models are estimated using maximum likelihood (ML) for most distributions
except the Gamma, Weibull and log-normal distributions for which maximum
simulated likelihood (MSL) is used. For this latter, several draws can be
implemented e.g., Halton, Generalized Halton, Sobol, or uniform. In the
case of uniform draws, antithetics can also be computed: first \code{Nsim/2}
draws are obtained, then the \code{Nsim/2} other draws are obtained as
counterpart of one (\code{1-draw}).

In the case of the truncated normal distribution, the density of the
convolution for each cross section is obtained as:

\deqn{
f(\bm{\epsilon}_i)=\frac{\sigma_*}{\sigma_u\sigma_v^T(2\pi)^{T/2}
\Phi\left(\frac{\mu}{\sigma_u}\right)}\exp{\left[-\frac{1}{2}\left(
-\frac{\mu_{i*}^2}{\sigma_*^2} + \frac{\sum_t\epsilon_{it}^2}{\sigma_v^2} + 
\frac{\mu^2}{\sigma_u^2}\right)\right]}\Phi\left(\frac{\mu_{i*}}{\sigma_*}
\right)
}

where

\deqn{\mu_{i*}=\frac{\mu\sigma_v^2-S\sigma_u^2\sum_t\epsilon_{it}}{
T\sigma_u^2+\sigma_v^2}}

\deqn{\sigma_*^2=\frac{\sigma_u^2\sigma_v^2}{T\sigma_u^2+\sigma_v^2}}

and \eqn{T} is the last period of observation of cross section \eqn{i}.

To account for heteroscedasticity in the variance parameters of the error
terms, a single part (right) formula can also be specified. To impose the
positivity to these parameters, the variances are modelled as:
\eqn{\sigma^2_{ui} = \exp{(\bm{\delta}'\mathbf{Z}_{ui})}} or
\eqn{\sigma^2_{vi} = \exp{(\bm{\phi}'\mathbf{Z}_{vi})}}, where
\eqn{\mathbf{Z}_{ui}} and \eqn{\mathbf{Z}_{vi}} are the heteroscedasticity
variables (inefficiency drivers in the case of \eqn{\mathbf{Z}_{ui}}) and
\eqn{\bm{\delta}} and \eqn{\bm{\phi}} the coefficients. In the case of
heterogeneity in the truncated mean \eqn{\mu_i}, it is modelled as
\eqn{\mu_i=\bm{\omega}'\mathbf{Z}_{\mu_i}}. The \eqn{\mathbf{Z}} variables
are such that they are constant over time for each cross section \eqn{i}.
When this is not the case, one of the \code{'invariance'} option is
activated.

In the case of the time varying inefficiency, the model writes as follows:

\deqn{y_{it} = \alpha + \mathbf{x_{it}^{\prime}}\bm{\beta} +
 v_{it} - SG(t)u_i}

where \eqn{G(t)} is defined following different specifications:

\deqn{\begin{array}{lll}
\text{References} & \text{Options in function} & \text{Formula} \\
\text{Kumbhakar (1990)} & \text{\code{'modelType = 'k90''}} & 
G(t)=\left[1+\exp{\left(\eta_1t + \eta_2t^2\right)}\right]^{-1}\\ 
\text{Battese and Coelli (1992)} & \text{\code{'modelType = 'bc92a''}} & 
G(t)=\exp{\left[-\eta(t-T)\right]} \\
\text{Cuesta and Orea (2002),  Feng and Serletis (2009)} & 
\text{\code{'modelType = 'bc92b''}} & 
G(t)=\exp{\left[-\eta_1(t-T)-\eta_2(t-T)^2\right]} \\
\text{Alvarez et al. (2006)} & \text{\code{'modelType = 'bc92c''}} & 
G(t)=\exp{\left(\bm{\eta}'\mathbf{Z}_{git}\right)} \\
\text{Battese and Coelli (1992)} & \text{\code{'modelType = 'mbc92''}} & 
G(t)= 1 + \eta_1(t-T) + \eta_2(t-T)^2\\
\text{Cuesta (2000)} & \text{\code{'modelType = 'c00''}} & 
G(t)=\exp{\left[-\eta_i(t-T)\right]} \\
\text{Kumbhakar and Wang (2005)} & \text{\code{'modelType = 'kw05''}} & 
G(t)=\exp{\left[-\eta(t-t_1)\right]} \\
\text{Modified Lee and Schmidt (1993)} & 
\text{\code{'modelType = 'mols93''}} & G(t)=\exp{\left[-\eta_t(t-T)\right]}
\end{array}
}

For the Modified Lee and Schmidt (1993) model, the last period parameter is not
identifiable so we set \eqn{\eta_T=1}.

In the case of truncated normal distribution, the density is similar to the
previous equation except that:

\deqn{\mu_{i*}=\frac{\mu\sigma_v^2-S\sigma_u^2\sum_tG(t)\epsilon_{it}}{\sigma_u^2\sum_tG(t)^2+\sigma_v^2}}

\deqn{\sigma_*^2=\frac{\sigma_u^2\sigma_v^2}{\sigma_u^2\sum_tG(t)^2+\sigma_v^2}}

In the case, \code{'modelType = 'k90''}, or \code{'modelType = 'bc92a''},or
\code{'modelType = 'bc92b''}, or \code{'modelType = 'mbc92''}, or
\code{'modelType = 'kw05''}, or \code{'modelType = 'mols93''}, specifying
\code{'muhet'} or \code{'uhet'} options does not have any impact in the
estimation because for these models, only the heteroscedasticity in the
two-sided error term is considered.

\code{sfapanel1} allows for the maximization of weighted log-likelihood.
When option \code{weights} is specified and \code{wscale = TRUE}, the weights
are scaled as

\deqn{new_{weights} = sample_{size} \times \frac{old_{weights}}{\sum(old_{weights})}}

For the weight variable, the \code{'invariance'} option is fist activated
before the scaling.

For complex problems, non-gradient methods (e.g. \code{nm} or \code{sann})
can be used to warm start the optimization and zoom in the neighborhood of
the solution. Then a gradient-based methods is recommended in the second
step. In the case of \code{sann}, we recommend to significantly increase the
iteration limit (e.g. \code{itermax = 20000}). The Conjugate Gradient
(\code{cg}) can also be used in the first stage.

A set of extractor functions for fitted model objects is available for
objects of class \code{'sfapanel1'} including methods to the generic functions
\code{\link[=print.sfapanel1]{print}}
}
\note{
For the Halton draws, the code is adapted from the \pkg{mlogit}
package.
}
\examples{
## Swiss railways data
# Pitt and Lee (1981) time invariant inefficiency
data <- swissrailways
# remove observations only present one year
data <- data[data$ID != names(which(table(data$ID) == 1)), ]
data <- plm::pdata.frame(data, index = c('ID', 'YEAR'))
res_pl81 <- sfapanel1(formula = LNCT ~ LNQ2 + LNQ3 + LNNET + LNPK + LNPL, 
modelType = 'pl81', S = -1, data = data)
}
\references{
Aigner, D., Lovell, C. A. K., and Schmidt, P. 1977. Formulation
and estimation of stochastic frontier production function models.
\emph{Journal of Econometrics}, \bold{6}(1), 21--37.

Alvarez, A., Amsler, C., Orea, L., & Schmidt, P. (2006).
Interpreting and Testing the Scaling Property in Models where Inefficiency
Depends on Firm Characteristics. \emph{Journal of Productivity Analysis},
\bold{25}(3), 201-212. doi: 10.1007/s11123-006-7639-3.

Battese, G. E., & Coelli, T. J. (1992). Frontier production
functions, technical efficiency and panel data: With application to paddy
farmers in India. \emph{Journal of Productivity Analysis}, \bold{3}(1-2),
153-169. doi: 10.1007/bf00158774.

Battese, G. E., and Coelli, T. J. 1995. A model for technical inefficiency
effects in a stochastic frontier production function for panel data.
\emph{Empirical Economics}, \bold{20}(2), 325--332.

Caudill, S. B., and Ford, J. M. 1993. Biases in frontier estimation due to
heteroscedasticity. \emph{Economics Letters}, \bold{41}(1), 17--20.

Caudill, S. B., Ford, J. M., and Gropper, D. M. 1995. Frontier estimation
and firm-specific inefficiency measures in the presence of
heteroscedasticity. \emph{Journal of Business & Economic Statistics},
\bold{13}(1), 105--111.

Cuesta, R. A. (2000). A production model with firm-specific temporal
variation in technical inefficiency: With application to Spanish dairy farms.
\emph{Journal of Productivity Analysis}, \bold{13}(2), 139-158.
doi: Doi 10.1023/A:1017297831646.

Cuesta, R. A., & Orea, L. (2002). Mergers and technical efficiency in Spanish
savings banks: A stochastic distance function approach.
\emph{Journal of Banking & Finance}, \bold{26}(12), 2231-2247.
Doi 10.1016/S0378-4266(01)00184-4.

Feng, G., & Serletis, A. (2009). Efficiency and productivity of the US
banking industry, 1998–2005: evidence from the Fourier cost function
satisfying global regularity conditions.
\emph{Journal of Applied Econometrics}, \bold{24}(1), 105-138.
doi: https://doi.org/10.1002/jae.1021.

Greene, W. H. 2003. Simulated likelihood estimation of the normal-Gamma
stochastic frontier function. \emph{Journal of Productivity Analysis},
\bold{19}(2-3), 179--190.

Hadri, K. 1999. Estimation of a doubly heteroscedastic stochastic frontier
cost function. \emph{Journal of Business & Economic Statistics},
\bold{17}(3), 359--363.

Hajargasht, G. 2015. Stochastic frontiers with a Rayleigh distribution.
\emph{Journal of Productivity Analysis}, \bold{44}(2), 199--208.

Huang, C. J., and Liu, J.-T. 1994. Estimation of a non-neutral stochastic
frontier production function. \emph{Journal of Productivity Analysis},
\bold{5}(2), 171--180.

Kumbhakar, S. C. (1990). Production Frontiers, Panel Data, and Time-Varying
Technical Inefficiency. \emph{Journal of Econometrics}, \bold{46}(1-2),
201-211. doi: Doi 10.1016/0304-4076(90)90055-X.

Kumbhakar, S. C., Ghosh, S., and McGuckin, J. T. 1991) A generalized
production frontier approach for estimating determinants of inefficiency in
U.S. dairy farms. \emph{Journal of Business & Economic Statistics},
\bold{9}(3), 279--286.

Kumbhakar, S. C., & Wang, H.-J. (2005). Estimation of growth convergence
using a stochastic production frontier approach.
\emph{Economics Letters}, \bold{88}(3), 300-305.
doi: https://doi.org/10.1016/j.econlet.2005.01.023.

Li, Q. 1996. Estimating a stochastic production frontier when the adjusted
error is symmetric. \emph{Economics Letters}, \bold{52}(3), 221--228.

Meeusen, W., and Vandenbroeck, J. 1977. Efficiency estimation from
Cobb-Douglas production functions with composed error. \emph{International
Economic Review}, \bold{18}(2), 435--445.

Migon, H. S., and Medici, E. V. 2001. Bayesian hierarchical models for
stochastic production frontier. Lacea, Montevideo, Uruguay.

Nguyen, N. B. 2010. Estimation of technical efficiency in stochastic
frontier analysis. PhD dissertation, Bowling Green State University, August.

Pitt, M. M., & Lee, L. F. (1981). The Measurement and Sources of Technical
Inefficiency in the Indonesian Weaving Industry.
\emph{Journal of Development Economics}, \bold{9}(1), 43-64.
doi: Doi 10.1016/0304-3878(81)90004-3.

Reifschneider, D., and Stevenson, R. 1991. Systematic departures from the
frontier: A framework for the analysis of firm inefficiency.
\emph{International Economic Review}, \bold{32}(3), 715--723.

Stevenson, R. E. 1980. Likelihood Functions for Generalized Stochastic
Frontier Estimation. \emph{Journal of Econometrics}, \bold{13}(1), 57--66.

Tsionas, E. G. 2007. Efficiency measurement with the Weibull stochastic
frontier. \emph{Oxford Bulletin of Economics and Statistics}, \bold{69}(5),
693--706.

Wang, H.J., and Schmidt, P. 2002. One-step and two-step estimation of the
effects of exogenous variables on technical efficiency levels. \emph{Journal
of Productivity Analysis}, \bold{18}:129--144.

Wang, K., and Ye, X. 2020. Development of alternative stochastic frontier
models for estimating time-space prism vertices. \emph{Transportation}.
}
\seealso{
\code{\link[=print.sfapanel1]{print}} for printing \code{sfapanel1}
object.
}
